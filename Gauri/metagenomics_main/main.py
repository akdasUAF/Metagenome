from decompose_graph import *
from contig_formation import *
import os
import time
from datetime import datetime, timedelta
from collections import defaultdict

class TimingLogger:
    """Class to handle timing and logging of pipeline steps."""
    
    def __init__(self):
        self.step_times = {}
        self.step_start_time = None
        self.total_start_time = None
        
    def start_total_timing(self):
        """Start timing the entire process."""
        self.total_start_time = time.time()
        
    def start_step(self, step_name):
        """Start timing a specific step."""
        self.step_start_time = time.time()
        print(f"Starting: {step_name}...")
        
    def end_step(self, step_name):
        """End timing a specific step and log the duration."""
        if self.step_start_time is None:
            print(f"Warning: No start time recorded for {step_name}")
            return 0
            
        duration = time.time() - self.step_start_time
        self.step_times[step_name] = duration
        print(f"Completed: {step_name} - Duration: {self.format_time(duration)}")
        return duration
        
    def get_total_time(self):
        """Get total execution time."""
        if self.total_start_time is None:
            return 0
        return time.time() - self.total_start_time
        
    @staticmethod
    def format_time(seconds):
        """Format time in human-readable format."""
        if seconds < 60:
            return f"{seconds:.2f} seconds"
        elif seconds < 3600:
            minutes = int(seconds // 60)
            secs = seconds % 60
            return f"{minutes}m {secs:.2f}s"
        else:
            hours = int(seconds // 3600)
            minutes = int((seconds % 3600) // 60)
            secs = seconds % 60
            return f"{hours}h {minutes}m {secs:.2f}s"

def calculate_n50(contigs):
    """
    Calculate N50 for a list of contigs.
    
    Args:
        contigs: List of contig sequences (strings)
    
    Returns:
        tuple: (n50_value, total_length, num_contigs)
    """
    if not contigs:
        return 0, 0, 0
    
    # Calculate lengths and sort in descending order
    contig_lengths = sorted([len(contig) for contig in contigs], reverse=True)
    total_length = sum(contig_lengths)
    
    # Find N50
    cumulative_length = 0
    n50_value = 0
    
    for length in contig_lengths:
        cumulative_length += length
        if cumulative_length >= total_length / 2:
            n50_value = length
            break
    
    return n50_value, total_length, len(contigs)

def calculate_assembly_stats(contigs):
    """
    Calculate comprehensive assembly statistics.
    
    Args:
        contigs: List of contig sequences
    
    Returns:
        dict: Dictionary containing various assembly statistics
    """
    if not contigs:
        return {
            'n50': 0,
            'total_length': 0,
            'num_contigs': 0,
            'mean_length': 0,
            'median_length': 0,
            'min_length': 0,
            'max_length': 0,
            'n90': 0,
            'l50': 0,
            'l90': 0
        }
    
    contig_lengths = sorted([len(contig) for contig in contigs], reverse=True)
    total_length = sum(contig_lengths)
    num_contigs = len(contig_lengths)
    
    # Calculate N50 and L50
    cumulative_length = 0
    n50_value = 0
    l50_count = 0
    
    for i, length in enumerate(contig_lengths):
        cumulative_length += length
        if cumulative_length >= total_length / 2:
            n50_value = length
            l50_count = i + 1
            break
    
    # Calculate N90 and L90
    cumulative_length = 0
    n90_value = 0
    l90_count = 0
    
    for i, length in enumerate(contig_lengths):
        cumulative_length += length
        if cumulative_length >= total_length * 0.9:
            n90_value = length
            l90_count = i + 1
            break
    
    # Calculate median
    median_length = contig_lengths[num_contigs // 2] if num_contigs > 0 else 0
    
    return {
        'n50': n50_value,
        'n90': n90_value,
        'l50': l50_count,
        'l90': l90_count,
        'total_length': total_length,
        'num_contigs': num_contigs,
        'mean_length': total_length / num_contigs if num_contigs > 0 else 0,
        'median_length': median_length,
        'min_length': min(contig_lengths) if contig_lengths else 0,
        'max_length': max(contig_lengths) if contig_lengths else 0
    }

def analyze_contigs_from_files(output_dir="output"):
    """
    Analyze contigs from the output files generated by your assembly pipeline.
    
    Args:
        output_dir: Directory containing the contig files
    
    Returns:
        dict: Analysis results for each cluster and overall
    """
    results = {}
    all_contigs = []
    
    if not os.path.exists(output_dir):
        print(f"Output directory '{output_dir}' not found.")
        return results
    
    # Process each contig file
    for filename in os.listdir(output_dir):
        if filename.endswith("_contigs.txt"):
            cluster_id = filename.replace("_contigs.txt", "")
            filepath = os.path.join(output_dir, filename)
            
            try:
                with open(filepath, 'r') as f:
                    contigs = [line.strip() for line in f if line.strip()]
                
                if contigs:
                    stats = calculate_assembly_stats(contigs)
                    results[cluster_id] = stats
                    all_contigs.extend(contigs)
                    
                    print(f"\nCluster {cluster_id} Statistics:")
                    print(f"  N50: {stats['n50']} bp")
                    print(f"  Total Length: {stats['total_length']} bp")
                    print(f"  Number of Contigs: {stats['num_contigs']}")
                    print(f"  Mean Length: {stats['mean_length']:.1f} bp")
                    print(f"  Max Length: {stats['max_length']} bp")
                    print(f"  L50: {stats['l50']} contigs")
                    
            except FileNotFoundError:
                print(f"File not found: {filepath}")
            except Exception as e:
                print(f"Error processing {filepath}: {e}")
    
    # Calculate overall statistics
    if all_contigs:
        overall_stats = calculate_assembly_stats(all_contigs)
        results['overall'] = overall_stats
        
        print(f"\nOverall Assembly Statistics:")
        print(f"  N50: {overall_stats['n50']} bp")
        print(f"  N90: {overall_stats['n90']} bp")
        print(f"  Total Length: {overall_stats['total_length']} bp")
        print(f"  Number of Contigs: {overall_stats['num_contigs']}")
        print(f"  Mean Length: {overall_stats['mean_length']:.1f} bp")
        print(f"  Median Length: {overall_stats['median_length']} bp")
        print(f"  Max Length: {overall_stats['max_length']} bp")
        print(f"  Min Length: {overall_stats['min_length']} bp")
        print(f"  L50: {overall_stats['l50']} contigs")
        print(f"  L90: {overall_stats['l90']} contigs")
    
    return results

def analyze_contigs_from_dict(contigs_by_subgraph):
    """
    Analyze contigs directly from the dictionary returned by generate_contigs().
    
    Args:
        contigs_by_subgraph: Dictionary with cluster_id as keys and list of contigs as values
    
    Returns:
        dict: Analysis results for each cluster and overall
    """
    results = {}
    all_contigs = []
    
    for cluster_id, contigs in contigs_by_subgraph.items():
        if contigs:
            stats = calculate_assembly_stats(contigs)
            results[cluster_id] = stats
            all_contigs.extend(contigs)
            
            print(f"\nCluster {cluster_id} Statistics:")
            print(f"  N50: {stats['n50']} bp")
            print(f"  Total Length: {stats['total_length']} bp")
            print(f"  Number of Contigs: {stats['num_contigs']}")
            print(f"  Mean Length: {stats['mean_length']:.1f} bp")
            print(f"  Max Length: {stats['max_length']} bp")
            print(f"  L50: {stats['l50']} contigs")
    
    # Calculate overall statistics
    if all_contigs:
        overall_stats = calculate_assembly_stats(all_contigs)
        results['overall'] = overall_stats
        
        print(f"\nOverall Assembly Statistics:")
        print(f"  N50: {overall_stats['n50']} bp")
        print(f"  N90: {overall_stats['n90']} bp")
        print(f"  Total Length: {overall_stats['total_length']} bp")
        print(f"  Number of Contigs: {overall_stats['num_contigs']}")
        print(f"  Mean Length: {overall_stats['mean_length']:.1f} bp")
        print(f"  Median Length: {overall_stats['median_length']} bp")
        print(f"  Max Length: {overall_stats['max_length']} bp")
        print(f"  Min Length: {overall_stats['min_length']} bp")
        print(f"  L50: {overall_stats['l50']} contigs")
        print(f"  L90: {overall_stats['l90']} contigs")
    
    return results

def write_stats_to_file(results, timing_logger, output_file="assembly_stats.txt"):
    """
    Write assembly statistics and timing information to a file.
    
    Args:
        results: Dictionary of assembly statistics
        timing_logger: TimingLogger instance with execution times
        output_file: Output file path
    """
    with open(output_file, 'w') as f:
        # Header with timestamp
        f.write("Assembly Statistics and Performance Report\n")
        f.write("=" * 60 + "\n")
        f.write(f"Generated on: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n")
        f.write("=" * 60 + "\n\n")
        
        # Execution timing section
        f.write("EXECUTION TIMING\n")
        f.write("-" * 30 + "\n")
        f.write(f"Total Pipeline Execution Time: {timing_logger.format_time(timing_logger.get_total_time())}\n\n")
        
        f.write("Step-by-Step Timing:\n")
        total_step_time = 0
        for step_name, duration in timing_logger.step_times.items():
            f.write(f"  {step_name}: {timing_logger.format_time(duration)}\n")
            total_step_time += duration
        
        f.write(f"\nSum of individual steps: {timing_logger.format_time(total_step_time)}\n")
        overhead = timing_logger.get_total_time() - total_step_time
        f.write(f"Pipeline overhead: {timing_logger.format_time(overhead)}\n\n")
        
        # Performance metrics
        f.write("PERFORMANCE METRICS\n")
        f.write("-" * 30 + "\n")
        if 'overall' in results:
            total_bp = results['overall']['total_length']
            total_time = timing_logger.get_total_time()
            if total_time > 0:
                f.write(f"Assembly rate: {total_bp/total_time:.0f} bp/second\n")
                f.write(f"Assembly rate: {(total_bp/total_time)/1000:.2f} kbp/second\n")
        f.write("\n")
        
        # Assembly statistics section
        f.write("ASSEMBLY STATISTICS\n")
        f.write("-" * 30 + "\n\n")
        
        for cluster_id, stats in results.items():
            f.write(f"Cluster: {cluster_id}\n")
            f.write(f"  N50: {stats['n50']} bp\n")
            f.write(f"  N90: {stats['n90']} bp\n")
            f.write(f"  Total Length: {stats['total_length']} bp\n")
            f.write(f"  Number of Contigs: {stats['num_contigs']}\n")
            f.write(f"  Mean Length: {stats['mean_length']:.1f} bp\n")
            f.write(f"  Median Length: {stats['median_length']} bp\n")
            f.write(f"  Max Length: {stats['max_length']} bp\n")
            f.write(f"  Min Length: {stats['min_length']} bp\n")
            f.write(f"  L50: {stats['l50']} contigs\n")
            f.write(f"  L90: {stats['l90']} contigs\n")
            f.write("\n")
        
        # Summary section
        f.write("PIPELINE SUMMARY\n")
        f.write("-" * 30 + "\n")
        f.write(f"Pipeline completed successfully in {timing_logger.format_time(timing_logger.get_total_time())}\n")
        if 'overall' in results:
            f.write(f"Total contigs generated: {results['overall']['num_contigs']}\n")
            f.write(f"Total assembly length: {results['overall']['total_length']} bp\n")
            f.write(f"Assembly N50: {results['overall']['n50']} bp\n")
    
    print(f"Comprehensive statistics and timing written to {output_file}")

def main_with_stats_and_timing():
    """
    Enhanced pipeline supporting:
    - Multi-round error correction
    - Progressive k-mer strategy
    - Iterative refinement and scaffolding
    - Graph simplification cycles
    - Full timing and assembly stats
    """
    timer = TimingLogger()
    timer.start_total_timing()

    print("=" * 60)
    print("STARTING ITERATIVE METAGENOME ASSEMBLY PIPELINE")
    print("=" * 60)

    # Parameters
    fastq_file = "/home/gauri/Metagenomics/Resources/16S_WT_day3_11_SRR2628505_1.fastq"
    k_values = [19, 29, 39, 49, 59, 69, 79, 89, 99] 
    # k_values = [99] 
    n_species = 2
    num_rounds = 1
    graph_cleanup_rounds = 1

    # Step 1: Extract paired-end reads
    timer.start_step("Extract paired-end reads")
    paired_end_reads = extract_paired_end_reads(fastq_file)
    timer.end_step("Extract paired-end reads")

    # Step 2: Parse raw reads
    timer.start_step("Parse reads")
    reads = parse_reads(fastq_file)
    timer.end_step("Parse reads")

    # Step 3: Initial error correction
    timer.start_step("Initial error correction")
    reads = error_substitution(reads, k_values[0], threshold=1.0)
    timer.end_step("Initial error correction")

    for k in k_values:
        print(f"\n--- Progressive Assembly with k={k} ---")

        # Prepare read chunks for Dask
        chunk_size = len(reads)
        read_chunks = [reads[i:i + chunk_size] for i in range(0, len(reads), chunk_size)]

        for round_num in range(num_rounds):
            print(f"\n=== COMPRESSION ROUND {round_num + 1} ===")

            # De Bruijn Graph construction (parallel)
            timer.start_step(f"k={k} R{round_num + 1}: Build graphs")
            graphs = compute(*[delayed(construct_de_bruijn_graph)(chunk, k) for chunk in read_chunks])
            graph = defaultdict(set)
            for g in graphs:
                for key, val in g.items():
                    graph[key].update(val)
            timer.end_step(f"k={k} R{round_num + 1}: Build graphs")

            # Annotate coverage and cluster
            timer.start_step(f"k={k} R{round_num + 1}: Coverage + Clustering")
            coverage_map = annotate_coverage(reads, k)
            kmers = list(coverage_map.keys())
            # clusters = cluster_by_tnf(kmers, n_species, method="euclidean")  # much faster  # or "mahalanobis"
            clusters = em_binning(kmers, coverage_map, n_bins=n_species)

            scaffolding_mode = round_num > 0
            clusters = adjust_clusters_with_paired_end(clusters, paired_end_reads, relax=scaffolding_mode)
            clusters, _ = resolve_ambiguous_nodes(clusters)
            subgraphs = extract_subgraphs(clusters, graph)
            timer.end_step(f"k={k} R{round_num + 1}: Coverage + Clustering")

        # Output subgraphs
        timer.start_step(f"k={k}: Output subgraphs")
        output_subgraphs(subgraphs, f"subgraphs_k{k}.txt")
        timer.end_step(f"k={k}: Output subgraphs")

        # Generate contigs
        timer.start_step(f"k={k}: Contig generation")
        contigs_by_subgraph = generate_contigs(subgraphs, cleanup_rounds=graph_cleanup_rounds)
        write_contigs_to_files(contigs_by_subgraph, output_dir=f"output_k{k}")
        timer.end_step(f"k={k}: Contig generation")

        # Stats
        timer.start_step(f"k={k}: Assembly stats")
        stats_results = analyze_contigs_from_dict(contigs_by_subgraph)
        write_stats_to_file(stats_results, timer, output_file=f"assembly_stats_k{k}.txt")
        timer.end_step(f"k={k}: Assembly stats")

    print("\nPIPELINE COMPLETED")
    print(f"Total time: {timer.format_time(timer.get_total_time())}")
    return stats_results, timer

def analyze_existing_with_timing(output_dir="output"):
    """
    Analyze existing contig files with timing information.
    """
    timer = TimingLogger()
    timer.start_total_timing()
    
    timer.start_step("Analyze existing contigs")
    results = analyze_contigs_from_files(output_dir)
    timer.end_step("Analyze existing contigs")
    
    timer.start_step("Write analysis results")
    write_stats_to_file(results, timer, "existing_contigs_analysis.txt")
    timer.end_step("Write analysis results")
    
    print(f"Analysis completed in {timer.format_time(timer.get_total_time())}")
    return results, timer

# Example usage:
if __name__ == "__main__":
    # Method 1: Run the complete pipeline with statistics and timing
    print("Running complete assembly pipeline with comprehensive timing...")
    results, timing_data = main_with_stats_and_timing()
    
    # Method 2: Analyze existing output files with timing
    print("\n" + "="*40)
    print("Analyzing existing contig files...")
    results_from_files, file_timing = analyze_existing_with_timing("output")