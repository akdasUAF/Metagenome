#!/bin/bash
#SBATCH --partition=t1small
#SBATCH --ntasks=1
#SBATCH --cpus-per-task=24         # Number of CPU cores for Herro
#SBATCH --mem=64G                  # Total memory for the job
#SBATCH --mail-user=wwinnett@alaska.edu
#SBATCH --mail-type=BEGIN,END,FAIL
#SBATCH --job-name="herro_single"
#SBATCH --output=logs/slurm_herro_single_%j.log
#SBATCH --error=logs/slurm_herro_single_err_%j.log

# --- Enable robust error handling for debugging ---
set -euxo pipefail # Exit immediately if a command exits with a non-zero status.
                   # Exit if any command in a pipeline fails.
                   # Treat unset variables as an error.
                   # 'e' is crucial for catching errors early.
                   # 'u' is crucial for catching undefined variables.
                   # 'x' prints commands and their arguments as they are executed.

echo "SLURM Job ID: ${SLURM_JOB_ID}"
echo "SLURM Node: $(hostname)"
echo "Current working directory: $(pwd)"
echo "Job started at: $(date)"

# --- Passed Arguments ---
# IMPORTANT: When running with sbatch your_script.sh arg1 arg2 arg3
# $1 = DATASET_ID (e.g., lr-even)
# $2 = RAW_READS_PATH (e.g., data/raw/lr-even/lr-even_raw.fastq) - this should be relative to ROOT_DIR
# $3 = TEST_ID (e.g., 1-5)
echo "Dataset ID: $1"
echo "Raw Reads Path (relative): $2"
echo "Test ID: $3"

# --- Declare Variables ---
# Use 'declare -r' where possible for constants
declare -r DATASET_ID="$1"
declare -r RAW_READS_PATH_REL="$2" # Renamed for clarity, it's relative
declare -r TEST_ID="$3"

# Determine ROOT_DIR from where sbatch was called
declare -r ROOT_DIR=$(pwd) # This is the directory where you ran 'sbatch'
echo "ROOT_DIR: ${ROOT_DIR}"

# --- Conda Initialization ---
echo "--- Initializing Conda ---"
CONDA_BASE=$(conda info --base)
if [ -f "${CONDA_BASE}/etc/profile.d/conda.sh" ]; then
    . "${CONDA_BASE}/etc/profile.d/conda.sh"
    echo "Conda initialized from ${CONDA_BASE}/etc/profile.d/conda.sh"
    conda activate herro # Explicitly activate for all commands, not just with conda run
    echo "Conda environment 'herro' activated."
else
    echo "ERROR: conda.sh not found at ${CONDA_BASE}/etc/profile.d/conda.sh."
    echo "Please ensure Conda is installed and 'conda init bash' has been run."
    exit 1
fi
echo "--- Conda Initialization Complete ---"

# --- Define Full Paths ---
# Absolute path to raw reads
declare -r RAW_READS_ABS_PATH="${ROOT_DIR}/${RAW_READS_PATH_REL}"
echo "Absolute Raw Reads Path: ${RAW_READS_ABS_PATH}"

# Base output directory for this specific Herro run
declare -r HERRO_RUN_OUT_DIR="${ROOT_DIR}/data/raw/${DATASET_ID}/test${TEST_ID}"
echo "Herro Run Output Directory: ${HERRO_RUN_OUT_DIR}"

# Paths for Herro components within your repository structure
declare -r HERRO_REPO_DIR="${ROOT_DIR}/tools/herro"
declare -r HERRO_MODEL="${HERRO_REPO_DIR}/model_R9_v0.1.pt"
declare -r HERRO_PREPROCESS_SCRIPT="${HERRO_REPO_DIR}/scripts/preprocess.sh"
declare -r HERRO_BATCH_ALIGN_SCRIPT="${HERRO_REPO_DIR}/scripts/create_batched_alignments.sh"
declare -r HERRO_SIG_BUILD="${HERRO_REPO_DIR}/herro.sif" # Path to your Singularity image

# Specific output paths for intermediate and final files
# Porechopped split FASTQ (output of preprocess.sh, input for alignment and inference)
declare -r PRE_PROCESSED_READS_PATH="${HERRO_RUN_OUT_DIR}/raw_preprocessed/duplex_tools_output_dir/porechopped_split.fastq.gz"
echo "Preprocessed Reads Path (for subsequent steps): ${PRE_PROCESSED_READS_PATH}"

# Read IDs file (output of seqkit, input for create_batched_alignments.sh)
declare -r READ_ID_PATH="${HERRO_RUN_OUT_DIR}/${DATASET_ID}_test${TEST_ID}_read_id.txt"
echo "Read ID File Path: ${READ_ID_PATH}"

# Batched alignments directory (output of create_batched_alignments.sh, input for inference)
declare -r BATCHED_ALIGNMENT_PATH="${HERRO_RUN_OUT_DIR}/herro_align_batches"
echo "Batched Alignment Directory: ${BATCHED_ALIGNMENT_PATH}"

# Final error-corrected FASTA output file
declare -r CORRECTED_FASTA_FILE="${HERRO_RUN_OUT_DIR}/herro_corrected_reads.fasta"
echo "Corrected FASTA Output File: ${CORRECTED_FASTA_FILE}"

# Parameters for Herro commands
declare -r NUM_THREADS="${SLURM_CPUS_PER_TASK}" # Use SLURM_CPUS_PER_TASK
declare -r BATCH_SIZE="64" # This is for -b in inference (batch_size)
declare -r FEAT_GEN_THREADS_PER_DEVICE="8" # For -t in inference (threads per GPU)
declare -r GPUS="0" # Example: use GPU 0. Adjust if you have multiple or specific IDs

# --- Ensure all necessary directories exist ---
echo "--- Creating Output Directories ---"
mkdir -p "$(dirname "${PRE_PROCESSED_READS_PATH}")" # For preprocess.sh output
mkdir -p "$(dirname "${READ_ID_PATH}")"             # For read IDs
mkdir -p "${BATCHED_ALIGNMENT_PATH}"                # For alignment batches
mkdir -p "$(dirname "${CORRECTED_FASTA_FILE}")"      # For final FASTA output
echo "--- Directories Created ---"

# --- Preprocess Reads (scripts/preprocess.sh) ---
echo "--- STEP 1: Starting Preprocessing with preprocess.sh ---"
echo "Command: conda run -n herro bash -c 'bash \"${HERRO_PREPROCESS_SCRIPT}\" \"${RAW_READS_ABS_PATH}\" \"${PRE_PROCESSED_READS_PATH%.fastq.gz}\" \"${NUM_THREADS}\" 2'"
conda run -n herro bash -c "bash '${HERRO_PREPROCESS_SCRIPT}' \
  '${RAW_READS_ABS_PATH}' \
  '${PRE_PROCESSED_READS_PATH%.fastq.gz}' \
  '${NUM_THREADS}' \
  '2'"
echo "--- STEP 1: Preprocessing Complete ---"
ls -lh "$(dirname "${PRE_PROCESSED_READS_PATH}")"

# --- 2. Generate Read IDs (using seqkit) ---
echo "--- STEP 2: Generating Read IDs with seqkit ---"
echo "Command: conda run -n herro bash -c 'seqkit seq -n -i \"${PRE_PROCESSED_READS_PATH}\" > \"${READ_ID_PATH}\"'"
conda run -n herro bash -c "seqkit seq -n -i '${PRE_PROCESSED_READS_PATH}' > '${READ_ID_PATH}'"
echo "--- STEP 2: Read IDs Generated ---"
ls -lh "${READ_ID_PATH}"

# --- 3. Create Batched Alignments (scripts/create_batched_alignments.sh) ---
echo "--- STEP 3: Creating Batched Alignments with create_batched_alignments.sh ---"
echo "Command: conda run -n herro bash -c 'bash \"${HERRO_BATCH_ALIGN_SCRIPT}\" \"${PRE_PROCESSED_READS_PATH}\" \"${READ_ID_PATH}\" \"${NUM_THREADS}\" \"${BATCHED_ALIGNMENT_PATH}\"'"
conda run -n herro bash -c "bash '${HERRO_BATCH_ALIGN_SCRIPT}' \
  '${PRE_PROCESSED_READS_PATH}' \
  '${READ_ID_PATH}' \
  '${NUM_THREADS}' \
  '${BATCHED_ALIGNMENT_PATH}'"
echo "--- STEP 3: Batched Alignments Created ---"
ls -lh "${BATCHED_ALIGNMENT_PATH}"

# --- 4. Herro Error-Correction (Inference) ---
echo "--- STEP 4: Starting Herro Inference ---"

# --- NEW: Define the path inside the container ---
# This will be the mount point for your ROOT_DIR inside the Singularity container.
declare -r CONTAINER_ROOT_DIR="/data" # A common choice, or /mnt/project, etc.

# --- NEW: Construct the Singularity bind mount option ---
declare -r SINGULARITY_BIND_OPT="--bind ${ROOT_DIR}:${CONTAINER_ROOT_DIR}"

# --- NEW: Adjust paths for Herro Inference to be relative to CONTAINER_ROOT_DIR ---
# HERRO_SIG_BUILD is still the host path because 'singularity run' needs it to find the SIF.
# All other paths become relative to CONTAINER_ROOT_DIR inside the container.
declare -r HERRO_MODEL_IN_CONTAINER="${CONTAINER_ROOT_DIR}/tools/herro/model_R9_v0.1.pt"
declare -r BATCHED_ALIGNMENT_PATH_IN_CONTAINER="${CONTAINER_ROOT_DIR}/data/raw/${DATASET_ID}/test${TEST_ID}/herro_align_batches"
declare -r PRE_PROCESSED_READS_PATH_IN_CONTAINER="${CONTAINER_ROOT_DIR}/data/raw/${DATASET_ID}/test${TEST_ID}/raw_preprocessed/duplex_tools_output_dir/porechopped_split.fastq.gz"
declare -r CORRECTED_FASTA_FILE_IN_CONTAINER="${CONTAINER_ROOT_DIR}/data/raw/${DATASET_ID}/test${TEST_ID}/herro_corrected_reads.fasta"


HERRO_INFERENCE_COMMAND="singularity run --nv ${SINGULARITY_BIND_OPT} \
  '${HERRO_SIG_BUILD}' inference \
  --read-alns '${BATCHED_ALIGNMENT_PATH_IN_CONTAINER}' \
  -t '${FEAT_GEN_THREADS_PER_DEVICE}' \
  -d '${GPUS}' \
  -m '${HERRO_MODEL_IN_CONTAINER}' \
  -b '${BATCH_SIZE}' \
  '${PRE_PROCESSED_READS_PATH_IN_CONTAINER}' \
  '${CORRECTED_FASTA_FILE_IN_CONTAINER}'"

echo "Full Herro Inference Command: ${HERRO_INFERENCE_COMMAND}"

# Execute the Herro Inference command
conda run -n herro bash -c "${HERRO_INFERENCE_COMMAND}"

echo "--- STEP 4: Herro Inference Complete ---"
ls -lh "${CORRECTED_FASTA_FILE}" # Check host path for existence and size

echo "--- Full Herro pipeline job finished successfully! ---"
echo "Job finished at: $(date)"